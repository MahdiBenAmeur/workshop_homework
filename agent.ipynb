{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing the needed liberaries and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import subprocess\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser ,StrOutputParser\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agents setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=1.0,  \n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm doing well, thank you for asking.\\n\\nHow may I help you today?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"Hello, how are you?\")\n",
    "response.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pydantic response structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnitTestResponse(BaseModel):\n",
    "    test_file_name : str = Field(description=\"The name of the test file.\")\n",
    "    tests: str = Field(description=\"Pytest test file content as plain code only.\")\n",
    "\n",
    "\n",
    "class ReadmeResponse(BaseModel):\n",
    "    readme: str = Field(\n",
    "        description=\"Complete README.md content as plain Markdown.\"\n",
    "    )\n",
    "class FiledocumentationResponse(BaseModel):\n",
    "    file_documentation: str = Field(\n",
    "        description=\"Technical documentation for the file. MUST explicitly include the file name.\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test_addition.py', 'import pytest\\nfrom exemple import addition\\n\\n@pytest.mark.parametrize(\\n    \"a, b, expected\",\\n    [\\n        (1, 2, 3),            # Positive integers\\n        (-1, -2, -3),         # Negative integers\\n        (1, -2, -1),          # Mixed sign integers\\n        (0, 5, 5),            # Zero with positive\\n        (-5, 0, -5),          # Zero with negative\\n        (0, 0, 0),            # Both zeros\\n        (1.5, 2.5, 4.0),      # Floating point numbers\\n        (1, 2.5, 3.5),        # Mixed integer and float\\n        (1000000, 2000000, 3000000), # Large integers\\n        (-100, 100, 0),       # Inverse numbers\\n        (3.14159, 2.71828, 5.85987), # More complex floats\\n    ],\\n)\\ndef test_addition_valid_inputs(a, b, expected):\\n    \"\"\"Test addition with various valid numerical inputs.\"\"\"\\n    assert addition(a, b) == pytest.approx(expected)\\n\\n# Although the original function is simple and doesn\\'t explicitly raise\\n# exceptions for non-numeric types (it would raise TypeError by Python\\'s \\'+\\' operator),\\n# if the intent was to strictly enforce numeric types, we might add a test like below.\\n# However, given the simplicity, Python\\'s default behavior is often acceptable.\\n# For the sake of demonstrating exception testing if it were a custom implementation.\\n# def test_addition_non_numeric_inputs():\\n#     \"\"\"Test addition with non-numeric inputs (should raise TypeError).\"\"\"\\n#     with pytest.raises(TypeError):\\n#         addition(\"hello\", \"world\")\\n#     with pytest.raises(TypeError):\\n#         addition(1, \"world\")\\n#     with pytest.raises(TypeError):\\n#         addition([1], [2]) # Note: list addition is concatenation, but for typical \\'add\\' it\\'s unexpected\\n')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_unit_test(file_name : str ,content: str , llm=model) -> str:\n",
    "\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=UnitTestResponse)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=(\n",
    "            \"You are a senior Python test engineer. Generate pytest unit tests for the Python code I provide.\\n\"\n",
    "            \"Constraints:\\n\"\n",
    "            \"- Output only the test filename and the test code (no explanations).\\n\"\n",
    "            \"- Deterministic: no network, no sleeps, no randomness unless seeded.\\n\"\n",
    "            \"- No real filesystem side effects: use tmp_path.\\n\"\n",
    "            \"- Prefer unit tests over integration tests.\\n\"\n",
    "            \"Coverage goals:\\n\"\n",
    "            \"- Test main public functions/classes.\\n\"\n",
    "            \"- Include  edge cases, and failure cases (assert exceptions) only if needed.\\n\"\n",
    "            \"- Hit important branches.\\n\\n\"\n",
    "            \"{format_instructions}\\n\\n\"\n",
    "            \"python file name {file_name}\\n\"\n",
    "            \"Python code to test:\\n\"\n",
    "            \"```python\\n{content}\\n```\"\n",
    "        ),\n",
    "        input_variables=[\"content\" , \"file_name\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "    resp: UnitTestResponse = chain.invoke({\"content\": content , \"file_name\": file_name})\n",
    "    return resp.test_file_name ,resp.tests\n",
    "\n",
    "\n",
    "example_content = \"\"\"\n",
    "def addition(a, b):\n",
    "    return a + b\n",
    "\"\"\"\n",
    "\n",
    "print(generate_unit_test(\"exemple.py\" ,example_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Technical Documentation for `exemple.py`\n",
      "\n",
      "### File Name\n",
      "`exemple.py`\n",
      "\n",
      "### Purpose of this File\n",
      "This file serves as a minimalist module providing a fundamental utility function for performing basic arithmetic addition. Its primary purpose is to encapsulate a single, straightforward operation, demonstrating the definition of a simple, reusable function in Python.\n",
      "\n",
      "### Main Functions\n",
      "\n",
      "#### `addition(a, b)`\n",
      "*   **Description**: This function is designed to take two arguments, `a` and `b`, and return their arithmetic sum.\n",
      "*   **Parameters**:\n",
      "    *   `a`: The first operand for the addition. Expected to be a numeric type (e.g., `int`, `float`) or any type that supports the `+` operator.\n",
      "    *   `b`: The second operand for the addition. Expected to be a numeric type (e.g., `int`, `float`) or any type that supports the `+` operator.\n",
      "*   **Returns**:\n",
      "    *   The result of `a + b`. The return type will typically be consistent with the input types (e.g., `int` if both `a` and `b` are `int`, `float` if either `a` or `b` is `float`).\n",
      "\n",
      "### Key Behaviors and Assumptions\n",
      "*   **Core Behavior**: The `addition` function relies directly on Python's built-in `+` operator to perform the summation. This means it supports standard numerical addition as well as other operations defined for the `+` operator on various data types (e.g., string concatenation, list merging).\n",
      "*   **Input Type Flexibility**: While primarily intended for numerical types, the function will operate correctly with any two types `a` and `b` for which the `+` operator is defined (e.g., `\"hello\" + \" world\"`).\n",
      "*   **Lack of Type Validation**: There is no explicit type checking or validation implemented within the `addition` function. If incompatible types are passed (e.g., `1 + [2]`), Python's default behavior for the `+` operator will result in a `TypeError` at runtime.\n",
      "*   **Stateless Operation**: The function is pure and stateless; it takes inputs, performs a calculation, and returns a result without side effects or maintaining internal state.\n",
      "*   **Minimalist Design**: The file is intentionally kept simple, containing only the function definition. It does not include complex logic, error handling beyond Python's default exceptions, or a `__main__` execution block.\n"
     ]
    }
   ],
   "source": [
    "def generate_file_documentation(file_name: str, content: str, llm=model) -> str:\n",
    "\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=FiledocumentationResponse)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=(\n",
    "            \"You are a senior Python engineer.\\n\"\n",
    "            \"Write clear technical documentation for the following Python file.\\n\\n\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Explicitly mention the file name: {file_name}\\n\"\n",
    "            \"- Explain the purpose of this file\\n\"\n",
    "            \"- Describe main functions/classes\\n\"\n",
    "            \"- Mention key behaviors and assumptions\\n\\n\"\n",
    "            \"{format_instructions}\\n\\n\"\n",
    "            \"File name: {file_name}\\n\\n\"\n",
    "            \"Python file content:\\n\"\n",
    "            \"```python\\n{content}\\n```\"\n",
    "        ),\n",
    "        input_variables=[\"file_name\", \"content\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "    resp: FiledocumentationResponse = chain.invoke(\n",
    "        {\"file_name\": file_name, \"content\": content}\n",
    "    )\n",
    "    return resp.file_documentation\n",
    "\n",
    "exemple_file_name = \"exemple.py\"\n",
    "example_content = \"\"\"\n",
    "def addition(a, b):\n",
    "    return a + b\n",
    "\"\"\"\n",
    "print(generate_file_documentation(exemple_file_name, example_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_readme(\n",
    "    project_structure: str,\n",
    "    file_docs: list[str],\n",
    "    llm=model,\n",
    ") -> str:\n",
    "\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=ReadmeResponse)\n",
    "\n",
    "    joined_file_docs = \"\\n\\n\".join(\n",
    "        f\"- {doc}\" for doc in file_docs\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=(\n",
    "            \"You are a senior software engineer.\\n\"\n",
    "            \"Generate a complete README.md for a Python project.\\n\\n\"\n",
    "            \"README must include:\\n\"\n",
    "            \"- Project overview\\n\"\n",
    "            \"- Project structure\\n\"\n",
    "            \"- Setup / installation\\n\"\n",
    "            \"- Usage\\n\\n\"\n",
    "            \"{format_instructions}\\n\\n\"\n",
    "            \"Project structure:\\n\"\n",
    "            \"{project_structure}\\n\\n\"\n",
    "            \"Files documentation:\\n\"\n",
    "            \"{file_docs}\\n\"\n",
    "            \"ALL IN MARKDOWN\"\n",
    "        ),\n",
    "        input_variables=[\"project_structure\", \"file_docs\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    res = chain.invoke(\n",
    "        {\n",
    "            \"project_structure\": project_structure,\n",
    "            \"file_docs\": joined_file_docs,\n",
    "        }\n",
    "    )\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get latest repo updates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_latest_repo_updates():\n",
    "    result = subprocess.run(['git', 'pull', 'origin', 'main'], \n",
    "                        capture_output=True, \n",
    "                        text=True)\n",
    "\n",
    "    print(result.stdout)  \n",
    "    print(result.stderr)  \n",
    "    print(result.returncode)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "push updates to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_updates_to_github(comments = \"\"):\n",
    "    result = subprocess.run(['git', 'add', '.'],)\n",
    "    result = subprocess.run(['git', 'commit', '-m', comments],)\n",
    "    result = subprocess.run(['git', 'push', 'origin', 'main'],)\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "    print(result.returncode)\n",
    "    return result.stdout\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get files to read and those to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def files_to_read():\n",
    "    ignore= files_dirs_to_ignore()\n",
    "    \n",
    "    def should_ignore(path):\n",
    "        return any(part in ignore for part in path.split(os.sep))\n",
    "    py_files = []\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        if should_ignore(root):\n",
    "            continue\n",
    "        for file in files:\n",
    "            \n",
    "\n",
    "            if should_ignore(file):\n",
    "                continue\n",
    "            if file.endswith('.py'):\n",
    "                py_files.append(os.path.join(root, file))\n",
    "    return py_files\n",
    "def files_dirs_to_ignore():\n",
    "    with open(\".gitignore\", \"r\") as f:\n",
    "        ignore = f.read().split(\"\\n\")\n",
    "    ignore = [i.strip() for i in ignore if i.strip()]\n",
    "    ignore.extend([\".git\", \"agent.ipynb\"])\n",
    "    return ignore\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get repo architecture for documentation later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "├── test_files/\n",
      "│   └── test_bank_simulator.py\n",
      "├── Bank_Simulator.py\n",
      "└── minibank.json\n"
     ]
    }
   ],
   "source": [
    "def get_git_structure():\n",
    "    ignore = files_dirs_to_ignore()\n",
    "    \n",
    "    def should_ignore(path):\n",
    "        return any(part in ignore for part in path.split(os.sep))\n",
    "    \n",
    "    structure = []\n",
    "    \n",
    "    def process_dir(current_dir, prefix=\"\"):\n",
    "        items = os.listdir(current_dir)\n",
    "        \n",
    "        dirs = [d for d in items if os.path.isdir(os.path.join(current_dir, d))]\n",
    "        files = [f for f in items if os.path.isfile(os.path.join(current_dir, f))]\n",
    "        \n",
    "        dirs.sort()\n",
    "        files.sort()\n",
    "        \n",
    "        for i, dir_name in enumerate(dirs):\n",
    "            if dir_name in ignore:\n",
    "                continue\n",
    "                \n",
    "            full_path = os.path.join(current_dir, dir_name)\n",
    "            if should_ignore(full_path):\n",
    "                continue\n",
    "                \n",
    "            is_last_dir = i == len(dirs) - 1 and not files\n",
    "            structure.append(f\"{prefix}{'└── ' if is_last_dir else '├── '}{dir_name}/\")\n",
    "            \n",
    "            extension = \"    \" if is_last_dir else \"│   \"\n",
    "            process_dir(full_path, prefix + extension)\n",
    "        \n",
    "        for i, file_name in enumerate(files):\n",
    "            if file_name in ignore or file_name.startswith('.'):\n",
    "                continue\n",
    "                \n",
    "            is_last = i == len(files) - 1\n",
    "            structure.append(f\"{prefix}{'└── ' if is_last else '├── '}{file_name}\")\n",
    "    \n",
    "    structure.append(\".\")\n",
    "    process_dir(\".\")\n",
    "    \n",
    "    return '\\n'.join(structure)\n",
    "\n",
    "\n",
    "print(get_git_structure())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assumulate the agent pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching the latest updates...\n",
      "getting the files to read...\n",
      "making test files dir...\n",
      "creating test files and documentation...\n",
      "generating readme file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def agent_pipeline():\n",
    "    # get latest repo updates\n",
    "    print(\"fetching the latest updates...\")\n",
    "    #get_latest_repo_updates()\n",
    "\n",
    "    # get files to read\n",
    "    print(\"getting the files to read...\")\n",
    "    files = files_to_read()\n",
    "    list_doc = []\n",
    "\n",
    "    # make test files dir\n",
    "    print(\"making test files dir...\")\n",
    "    os.makedirs(\"test_files\", exist_ok=True)\n",
    "    print(\"creating test files and documentation...\")\n",
    "    for file in files:\n",
    "        \n",
    "        with open(file, \"r\") as f:\n",
    "            content = f.read()\n",
    "        doc = generate_file_documentation(file, content)\n",
    "        list_doc.append(doc)\n",
    "        test_file_name , test_code = generate_unit_test(file, content)\n",
    "\n",
    "        with open(f\"test_files/{test_file_name}\", \"w\") as f:\n",
    "            f.write(test_code)\n",
    "    print(\"generating readme file\")\n",
    "    proj_structure = get_git_structure()\n",
    "    readme = generate_readme(proj_structure , list_doc)\n",
    "    with open(\"README.md\", \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    print(\"pushing updates to github\")\n",
    "    push_updates_to_github(\"adding test files and documentation\")\n",
    "    \n",
    "        \n",
    "agent_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
